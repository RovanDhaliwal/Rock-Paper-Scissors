{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RovanDhaliwal/Rock-Paper-Scissors/blob/main/VGP338_Rovan_Final_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project: Real-Time Object Detection System\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "Congratulations on completing the foundational modules! You've learned Python programming, data manipulation with NumPy and Pandas, built neural networks from scratch (MLPs), understood convolutional architectures (CNNs), and mastered transfer learning techniques. Now it's time to apply everything you've learned to build a **real-world object detection system**.\n",
        "\n",
        "In this final project, you will leverage **pre-trained models** to create an intelligent system that can detect and classify objects in **images or live video feeds**. Your system should demonstrate both **high accuracy** and **real-time performance**.\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By completing this project, you will:\n",
        "- Apply transfer learning to a real-world computer vision problem\n",
        "- Work with pre-trained models from PyTorch, Hugging Face, or other sources\n",
        "- Implement real-time video processing and object detection\n",
        "- Optimize models for both accuracy and performance\n",
        "- Build a complete end-to-end ML application\n",
        "- Present and document your work professionally\n",
        "\n",
        "---\n",
        "\n",
        "## Project Requirements\n",
        "\n",
        "### Core Requirements (Mandatory)\n",
        "\n",
        "#### 1. **Use a Pre-trained Model**\n",
        "You must use at least one pre-trained model from:\n",
        "- **PyTorch Hub** (torchvision.models, torch.hub)\n",
        "- **Hugging Face** (transformers, timm)\n",
        "- **TensorFlow Hub**\n",
        "- **ONNX Model Zoo**\n",
        "- Other reputable sources (YOLO, Detectron2, etc.)\n",
        "\n",
        "#### 2. **Object Detection Capability**\n",
        "Your system must be able to:\n",
        "- Detect objects in static images\n",
        "- Process live video feed from webcam OR video files\n",
        "- Draw bounding boxes around detected objects\n",
        "- Display class labels and confidence scores\n",
        "\n",
        "#### 3. **Performance Metrics**\n",
        "You must measure and report:\n",
        "- **Accuracy**: Precision, Recall, F1-Score, mAP (if applicable)\n",
        "- **Speed**: FPS (Frames Per Second), inference time\n",
        "- **Resource Usage**: CPU/GPU utilization, memory consumption\n",
        "\n",
        "#### 4. **User Interface**\n",
        "Implement at least one of:\n",
        "- Command-line interface with clear instructions\n",
        "- GUI using OpenCV, Tkinter, or Streamlit (Stretch Goals)\n",
        "- Web interface using Flask/FastAPI (Stretch Goals)\n",
        "- Jupyter notebook with interactive widgets\n",
        "\n",
        "#### 5. **Documentation**\n",
        "Provide comprehensive documentation including:\n",
        "- README with setup instructions\n",
        "- Code comments explaining key sections\n",
        "- Model selection justification\n",
        "- Performance analysis and results\n",
        "- Demo video or screenshots\n",
        "\n",
        "---\n",
        "\n",
        "## Project Ideas\n",
        "\n",
        "Choose **ONE** of the following project ideas or propose your own (subject to approval):\n",
        "\n",
        "### ü§ñ **1. Autonomous Robot Obstacle Detection**\n",
        "Build a system that detects obstacles for autonomous navigation:\n",
        "- Detect people, vehicles, furniture, walls\n",
        "- Calculate distance/proximity warnings\n",
        "- Real-time processing for navigation decisions\n",
        "- **Bonus**: Integrate with robot simulator (Gazebo, Webots)\n",
        "\n",
        "**Suggested Models**: YOLO, Faster R-CNN, MobileNet-SSD\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ü **2. Sign Language Translator**\n",
        "Create a real-time sign language recognition system:\n",
        "- Detect hand gestures from webcam\n",
        "- Translate ASL (American Sign Language) to text\n",
        "- Support alphabet and common words/phrases\n",
        "- **Bonus**: Text-to-speech output\n",
        "\n",
        "**Suggested Models**: MediaPipe Hands, Custom CNN with transfer learning, Vision Transformers\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úä‚úã‚úåÔ∏è **3. Unbeatable Rock-Paper-Scissors AI**\n",
        "Build an AI that predicts and beats human players:\n",
        "- Real-time hand gesture recognition\n",
        "- Predict opponent's move before they complete it\n",
        "- Track win/loss statistics\n",
        "- **Bonus**: Add pattern recognition to predict player tendencies\n",
        "\n",
        "**Suggested Models**: MobileNetV2, EfficientNet, Custom CNN\n",
        "\n",
        "---\n",
        "\n",
        "### üöó **4. Traffic Sign Detection & Recognition**\n",
        "Develop a system for autonomous driving assistance:\n",
        "- Detect and classify traffic signs (stop, yield, speed limit, etc.)\n",
        "- Work with various lighting and weather conditions\n",
        "- Real-time processing for driving scenarios\n",
        "- **Bonus**: Add lane detection\n",
        "\n",
        "**Suggested Models**: YOLO, Faster R-CNN, ResNet with transfer learning\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **5. Smart Surveillance System**\n",
        "Create an intelligent security monitoring system:\n",
        "- Detect people, vehicles, suspicious activities\n",
        "- Alert on specific events (person entering restricted area)\n",
        "- Track objects across frames\n",
        "- **Bonus**: Face recognition, anomaly detection\n",
        "\n",
        "**Suggested Models**: YOLO, Detectron2, RetinaNet\n",
        "\n",
        "---\n",
        "\n",
        "### üçé **6. Food Recognition & Nutrition Tracker**\n",
        "Build a dietary assistant application:\n",
        "- Identify food items from photos\n",
        "- Estimate portion sizes\n",
        "- Provide nutritional information\n",
        "- **Bonus**: Meal logging and calorie tracking\n",
        "\n",
        "**Suggested Models**: EfficientNet, Vision Transformers, Food-101 pre-trained models\n",
        "\n",
        "---\n",
        "\n",
        "### üêï **7. Pet Breed Classifier & Detector**\n",
        "Develop a pet identification system:\n",
        "- Detect cats/dogs in images or video\n",
        "- Classify breed with high accuracy\n",
        "- Provide breed information and characteristics\n",
        "- **Bonus**: Multi-pet detection in same frame\n",
        "\n",
        "**Suggested Models**: ResNet, EfficientNet, YOLO for detection\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **8. Custom Project (Your Idea)**\n",
        "Propose your own object detection project:\n",
        "- Must involve real-time or near-real-time processing\n",
        "- Must use pre-trained models with transfer learning\n",
        "- Must have clear accuracy and performance metrics\n",
        "- Submit a 1-page proposal for approval\n",
        "\n",
        "---\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "### Minimum Performance Targets\n",
        "\n",
        "| Metric             | Minimum Target | Excellent Target |\n",
        "| ------------------ | -------------- | ---------------- |\n",
        "| **Accuracy**       | 70%            | 90%+             |\n",
        "| **FPS (Video)**    | 10 FPS         | 30+ FPS          |\n",
        "| **Inference Time** | <200ms         | <50ms            |\n",
        "| **Model Size**     | <500MB         | <100MB           |\n",
        "\n",
        "### Required Technologies\n",
        "\n",
        "**Core Stack:**\n",
        "- Python 3.8+\n",
        "- PyTorch or TensorFlow\n",
        "- OpenCV for video processing\n",
        "- NumPy for data manipulation\n",
        "\n",
        "**Recommended Libraries:**\n",
        "```python\n",
        "# Computer Vision\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "\n",
        "# Pre-trained Models\n",
        "from transformers import pipeline  # Hugging Face\n",
        "import timm  # PyTorch Image Models\n",
        "\n",
        "# Utilities\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Performance\n",
        "import time\n",
        "from collections import deque\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Project Phases\n",
        "\n",
        "### Phase 1: Research & Planning\n",
        "- [ ] Choose your project idea\n",
        "- [ ] Research available pre-trained models\n",
        "- [ ] Identify datasets for testing/validation\n",
        "- [ ] Set up development environment\n",
        "---\n",
        "\n",
        "### Phase 2: Model Selection & Integration\n",
        "- [ ] Download and test pre-trained models\n",
        "- [ ] Implement basic inference pipeline\n",
        "- [ ] Test on sample images\n",
        "- [ ] Benchmark initial performance\n",
        "\n",
        "---\n",
        "\n",
        "### Phase 3: Real-Time Implementation\n",
        "- [ ] Implement video capture and processing\n",
        "- [ ] Optimize for real-time performance\n",
        "- [ ] Add visualization (bounding boxes, labels)\n",
        "- [ ] Implement FPS counter and metrics\n",
        "- [ ] Handle edge cases and errors\n",
        "\n",
        "---\n",
        "\n",
        "### Phase 4: Optimization & Enhancement\n",
        "- [ ] Fine-tune model if needed\n",
        "- [ ] Optimize inference speed\n",
        "- [ ] Improve accuracy through data augmentation\n",
        "- [ ] Add advanced features (tracking, alerts, etc.)\n",
        "- [ ] Conduct thorough testing\n",
        "\n",
        "---\n",
        "\n",
        "### Phase 5: Documentation & Presentation\n",
        "- [ ] Write comprehensive README\n",
        "- [ ] Create demo video\n",
        "- [ ] Prepare presentation slides\n",
        "- [ ] Document challenges and solutions\n",
        "- [ ] Prepare for final presentation\n",
        "\n",
        "---\n",
        "\n",
        "## Implementation Guide\n",
        "\n",
        "### Step 1: Set Up Your Environment\n",
        "\n",
        "```bash\n",
        "# Create virtual environment\n",
        "python -m venv venv\n",
        "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
        "\n",
        "# Install dependencies\n",
        "pip install torch torchvision opencv-python\n",
        "pip install transformers timm\n",
        "pip install numpy pandas matplotlib\n",
        "pip install streamlit  # Optional: for web UI\n",
        "```\n",
        "\n",
        "### Step 2: Load Pre-trained Model\n",
        "\n",
        "**Example with YOLO:**\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Load YOLOv5 from PyTorch Hub\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Test on image\n",
        "results = model('path/to/image.jpg')\n",
        "results.show()\n",
        "```\n",
        "\n",
        "**Example with Hugging Face:**\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load object detection pipeline\n",
        "detector = pipeline(\"object-detection\",\n",
        "                   model=\"facebook/detr-resnet-50\")\n",
        "\n",
        "# Detect objects\n",
        "results = detector(\"path/to/image.jpg\")\n",
        "print(results)\n",
        "```\n",
        "\n",
        "### Step 3: Implement Real-Time Video Processing\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Load model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Open webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# FPS calculation\n",
        "fps_queue = []\n",
        "\n",
        "while True:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Read frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    \n",
        "    # Run inference\n",
        "    results = model(frame)\n",
        "    \n",
        "    # Render results\n",
        "    rendered_frame = results.render()[0]\n",
        "    \n",
        "    # Calculate FPS\n",
        "    fps = 1 / (time.time() - start_time)\n",
        "    fps_queue.append(fps)\n",
        "    if len(fps_queue) > 30:\n",
        "        fps_queue.pop(0)\n",
        "    avg_fps = sum(fps_queue) / len(fps_queue)\n",
        "    \n",
        "    # Display FPS\n",
        "    cv2.putText(rendered_frame, f'FPS: {avg_fps:.1f}',\n",
        "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 255, 0), 2)\n",
        "    \n",
        "    # Show frame\n",
        "    cv2.imshow('Object Detection', rendered_frame)\n",
        "    \n",
        "    # Exit on 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "```\n",
        "\n",
        "### Step 4: Optimize for Performance\n",
        "\n",
        "**Use GPU Acceleration:**\n",
        "```python\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "```\n",
        "\n",
        "**Reduce Input Resolution:**\n",
        "```python\n",
        "# Resize frame for faster processing\n",
        "frame_resized = cv2.resize(frame, (640, 480))\n",
        "```\n",
        "\n",
        "**Use Lighter Models:**\n",
        "```python\n",
        "# YOLOv5n (nano) is faster than YOLOv5s (small)\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5n')\n",
        "```\n",
        "\n",
        "**Batch Processing (for video files):**\n",
        "```python\n",
        "# Process multiple frames at once\n",
        "results = model(frames_batch)\n",
        "```\n",
        "\n",
        "### Step 5: Measure Performance\n",
        "\n",
        "```python\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def benchmark_model(model, test_images, num_runs=100):\n",
        "    \"\"\"Benchmark model performance\"\"\"\n",
        "    inference_times = []\n",
        "    \n",
        "    for _ in range(num_runs):\n",
        "        start = time.time()\n",
        "        results = model(test_images)\n",
        "        inference_times.append(time.time() - start)\n",
        "    \n",
        "    return {\n",
        "        'mean_time': np.mean(inference_times),\n",
        "        'std_time': np.std(inference_times),\n",
        "        'fps': 1 / np.mean(inference_times)\n",
        "    }\n",
        "\n",
        "# Run benchmark\n",
        "stats = benchmark_model(model, test_image)\n",
        "print(f\"Average inference time: {stats['mean_time']*1000:.2f}ms\")\n",
        "print(f\"FPS: {stats['fps']:.1f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation Criteria\n",
        "\n",
        "Your project will be evaluated on the following criteria:\n",
        "\n",
        "### 1. **Technical Implementation (40 points)**\n",
        "- [ ] Correct use of pre-trained models (10 pts)\n",
        "- [ ] Real-time video processing capability (10 pts)\n",
        "- [ ] Code quality and organization (10 pts)\n",
        "- [ ] Error handling and robustness (10 pts)\n",
        "\n",
        "### 2. **Performance (25 points)**\n",
        "- [ ] Accuracy metrics (10 pts)\n",
        "- [ ] Speed/FPS performance (10 pts)\n",
        "- [ ] Resource efficiency (5 pts)\n",
        "\n",
        "### 3. **Innovation & Features (15 points)**\n",
        "- [ ] Creative problem-solving (5 pts)\n",
        "- [ ] Additional features beyond requirements (5 pts)\n",
        "- [ ] User experience and interface (5 pts)\n",
        "\n",
        "### 4. **Documentation (10 points)**\n",
        "- [ ] Clear README with setup instructions (3 pts)\n",
        "- [ ] Code comments and documentation (3 pts)\n",
        "- [ ] Performance analysis and results (4 pts)\n",
        "\n",
        "### 5. **Presentation (10 points)**\n",
        "- [ ] Clear explanation of approach (4 pts)\n",
        "- [ ] Live demo or video demonstration (4 pts)\n",
        "- [ ] Discussion of challenges and solutions (2 pts)\n",
        "\n",
        "**Total: 100 points**\n",
        "\n",
        "**Bonus Points (up to 20):**\n",
        "- Exceptional performance (>95% accuracy or >60 FPS)\n",
        "- Novel application or approach\n",
        "- Deployment (web app, mobile app, Docker container)\n",
        "- Contribution to open source or dataset creation\n",
        "\n",
        "---\n",
        "\n",
        "## Submission Requirements\n",
        "Githb Repo with README\n",
        "\n",
        "### README.md Template\n",
        "\n",
        "```markdown\n",
        "# [Project Title]\n",
        "\n",
        "## Overview\n",
        "Brief description of your project and its purpose.\n",
        "\n",
        "## Features\n",
        "- Feature 1\n",
        "- Feature 2\n",
        "- Feature 3\n",
        "\n",
        "## Installation\n",
        "\n",
        "### Prerequisites\n",
        "- Python 3.8+\n",
        "- Webcam (for real-time detection)\n",
        "\n",
        "### Setup\n",
        "\\`\\`\\`bash\n",
        "pip install -r requirements.txt\n",
        "\\`\\`\\`\n",
        "\n",
        "## Usage\n",
        "\n",
        "### Run on Images\n",
        "\\`\\`\\`bash\n",
        "python main.py --mode image --input path/to/image.jpg\n",
        "\\`\\`\\`\n",
        "\n",
        "### Run on Video\n",
        "\\`\\`\\`bash\n",
        "python main.py --mode video --input path/to/video.mp4\n",
        "\\`\\`\\`\n",
        "\n",
        "### Run on Webcam\n",
        "\\`\\`\\`bash\n",
        "python main.py --mode webcam\n",
        "\\`\\`\\`\n",
        "\n",
        "## Model Information\n",
        "- **Model**: [Model name and source]\n",
        "- **Architecture**: [Brief architecture description]\n",
        "- **Pre-training**: [What dataset was it pre-trained on]\n",
        "\n",
        "## Performance\n",
        "- **Accuracy**: XX%\n",
        "- **FPS**: XX\n",
        "- **Inference Time**: XXms\n",
        "\n",
        "## Results\n",
        "[Include screenshots or link to demo video]\n",
        "\n",
        "## Challenges & Solutions\n",
        "[Discuss main challenges and how you solved them]\n",
        "\n",
        "## Future Improvements\n",
        "[What would you add given more time]\n",
        "\n",
        "## Acknowledgments\n",
        "[Credit any resources, tutorials, or code you used]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Resources\n",
        "\n",
        "### Pre-trained Models\n",
        "\n",
        "**Object Detection:**\n",
        "- [YOLOv5](https://github.com/ultralytics/yolov5) - Fast and accurate\n",
        "- [Detectron2](https://github.com/facebookresearch/detectron2) - Facebook's detection framework\n",
        "- [Hugging Face DETR](https://huggingface.co/facebook/detr-resnet-50) - Transformer-based detection\n",
        "- [TorchVision Models](https://pytorch.org/vision/stable/models.html) - Faster R-CNN, RetinaNet\n",
        "\n",
        "**Classification (for specific tasks):**\n",
        "- [timm](https://github.com/rwightman/pytorch-image-models) - 700+ pre-trained models\n",
        "- [Hugging Face Vision Models](https://huggingface.co/models?pipeline_tag=image-classification)\n",
        "\n",
        "### Datasets for Testing\n",
        "\n",
        "- [COCO Dataset](https://cocodataset.org/) - Common objects\n",
        "- [Open Images](https://storage.googleapis.com/openimages/web/index.html) - Large-scale detection\n",
        "- [Roboflow Universe](https://universe.roboflow.com/) - Custom datasets\n",
        "- [Kaggle Datasets](https://www.kaggle.com/datasets) - Various domains\n",
        "\n",
        "### Tutorials & Documentation\n",
        "\n",
        "- [PyTorch Object Detection Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
        "- [OpenCV Python Tutorials](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)\n",
        "- [Hugging Face Vision Tasks](https://huggingface.co/tasks/object-detection)\n",
        "- [Real Python - OpenCV](https://realpython.com/face-recognition-with-python/)\n",
        "\n",
        "---\n",
        "\n",
        "## Tips for Success\n",
        "\n",
        "### üéØ **Start Simple**\n",
        "1. Get a basic model working on static images first\n",
        "2. Then add video processing\n",
        "3. Finally optimize for performance\n",
        "\n",
        "### ‚ö° **Optimize Early**\n",
        "- Profile your code to find bottlenecks\n",
        "- Use GPU acceleration when available\n",
        "- Consider model quantization for speed\n",
        "\n",
        "### üìä **Measure Everything**\n",
        "- Track FPS, inference time, accuracy\n",
        "- Compare different models\n",
        "- Document what works and what doesn't\n",
        "\n",
        "### üêõ **Debug Systematically**\n",
        "- Test each component separately\n",
        "- Use print statements and logging\n",
        "- Validate input/output at each stage\n",
        "\n",
        "### üí° **Be Creative**\n",
        "- Add unique features that showcase your skills\n",
        "- Think about real-world applications\n",
        "- Make it visually appealing\n",
        "\n",
        "### ü§ù **Ask for Help**\n",
        "- Use office hours for technical questions\n",
        "- Collaborate with classmates (but submit individual work)\n",
        "- Search Stack Overflow and GitHub issues\n",
        "\n",
        "---\n",
        "\n",
        "## Presentation Guidelines\n",
        "\n",
        "Your final presentation should be **5 minutes** and include:\n",
        "\n",
        "1. **Introduction**\n",
        "   - Problem statement\n",
        "   - Why this project matters\n",
        "\n",
        "2. **Technical Approach**\n",
        "   - Model selection and justification\n",
        "   - Architecture overview\n",
        "   - Key implementation decisions\n",
        "\n",
        "3. **Live Demo**\n",
        "   - Show your system in action\n",
        "   - Demonstrate key features\n",
        "   - Highlight performance metrics\n",
        "\n",
        "4. **Results & Analysis**\n",
        "   - Accuracy and performance metrics\n",
        "   - Comparison with baselines\n",
        "   - Challenges and solutions\n",
        "\n",
        "5. **Q&A**\n",
        "   - Answer questions from audience\n",
        "\n",
        "---\n",
        "\n",
        "## Frequently Asked Questions\n",
        "\n",
        "**Q: Can I use multiple pre-trained models?**  \n",
        "A: Yes! Combining models (e.g., detection + classification) is encouraged.\n",
        "\n",
        "**Q: What if I don't have a GPU?**  \n",
        "A: Use Google Colab, Kaggle Notebooks, or optimize for CPU with lighter models.\n",
        "\n",
        "**Q: Can I work in a team?**  \n",
        "A: This is an individual project, but you can discuss ideas with classmates.\n",
        "\n",
        "**Q: What if my accuracy is below 70%?**  \n",
        "A: Document why and what you tried. Focus on learning and problem-solving.\n",
        "\n",
        "**Q: Can I use a dataset I created?**  \n",
        "A: Yes! Custom datasets are encouraged, especially for unique applications.\n",
        "\n",
        "**Q: How do I handle webcam on remote servers?**  \n",
        "A: Test locally or use video files or image sequence. Document any limitations.\n",
        "\n",
        "---\n",
        "\n",
        "## Academic Integrity\n",
        "\n",
        "- You must write your own code (no copy-pasting entire projects)\n",
        "- Properly cite any code snippets, tutorials, or resources used\n",
        "- Pre-trained models are allowed and encouraged\n",
        "- Using libraries and frameworks is expected\n",
        "- Collaboration on ideas is okay, but implementation must be individual\n",
        "\n",
        "---\n",
        "\n",
        "## Final Notes\n",
        "\n",
        "This project is your opportunity to showcase everything you've learned and build something impressive for your portfolio. Choose a project you're passionate about, aim for excellence, and don't be afraid to be creative!\n",
        "\n",
        "Remember:\n",
        "- **Start early** - Don't underestimate the time needed\n",
        "- **Iterate often** - Get feedback and improve continuously  \n",
        "- **Document everything** - Your future self will thank you\n",
        "- **Have fun** - This is your chance to build something cool!\n",
        "\n",
        "Good luck! üöÄ\n"
      ],
      "metadata": {
        "id": "_XPSNpIr-SzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fJJntS2l-SxK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6EBdMDDjy6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.models as models\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import torch.optim as optim\n",
        "\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "BJ1X9WBqbIUo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "\n",
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors\", data_dir=\"data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rgyyFGubNgR",
        "outputId": "70ab1ab6-6fac-48c6-b0a8-b3fd9a8a0d3a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: rovandhaliwal\n",
            "Your Kaggle Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Dataset URL: https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors\n",
            "Downloading rockpaperscissors.zip to data/rockpaperscissors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 306M/306M [00:06<00:00, 47.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize(244),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose({\n",
        "    transforms.Resize(244),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "})"
      ],
      "metadata": {
        "id": "ojOL-MaUcB36"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "444c2dd2"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize(244),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(244),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "571c74ff"
      },
      "source": [
        "root = Path(\"data/rockpaperscissors/rps-cv-images\")\n",
        "\n",
        "base = datasets.ImageFolder(root=str(root), transform=None)\n",
        "\n",
        "n = len(base)\n",
        "perm = torch.randperm(n)\n",
        "n_train = int(n * 0.8)\n",
        "idx_train = perm[:n_train]\n",
        "idx_test = perm[n_train:]\n",
        "\n",
        "train_base = ImageFolder(root=str(root), transform=train_transform)\n",
        "test_base = ImageFolder(root=str(root), transform=test_transform)\n",
        "\n",
        "train_dataset = Subset(train_base, idx_train)\n",
        "test_dataset = Subset(test_base, idx_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268fc030"
      },
      "source": [
        "alex_net_model.classifier = nn.Sequential(nn.Linear(9216, 1024),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(0.4),\n",
        "                                 nn.Linear(1024, 3), # Changed from 2 to 3 classes\n",
        "                                 nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ede4372b"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alex_net_model.classifier.parameters(), lr=0.001)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2d16a6a",
        "outputId": "d614c648-4600-484e-fe40-788956f1359a"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    alex_net_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(train_loader, start=1):\n",
        "        inputs, labels = batch\n",
        "\n",
        "        output = alex_net_model(inputs)\n",
        "        loss = criterion(output, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print batch loss\n",
        "        if i % 20 == 0:\n",
        "            print(f'Batch {i} loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 20 loss: 0.5502\n",
            "Batch 40 loss: 0.1981\n",
            "Epoch 1/10, Loss: 0.2866\n",
            "Batch 20 loss: 0.0038\n",
            "Batch 40 loss: 0.0308\n",
            "Epoch 2/10, Loss: 0.0758\n",
            "Batch 20 loss: 0.0005\n",
            "Batch 40 loss: 0.0034\n",
            "Epoch 3/10, Loss: 0.0470\n",
            "Batch 20 loss: 0.0000\n",
            "Batch 40 loss: 0.0001\n",
            "Epoch 4/10, Loss: 0.0280\n",
            "Batch 20 loss: 0.0091\n",
            "Batch 40 loss: 0.0000\n",
            "Epoch 5/10, Loss: 0.0191\n",
            "Batch 20 loss: 0.0018\n",
            "Batch 40 loss: 0.0000\n",
            "Epoch 6/10, Loss: 0.0166\n",
            "Batch 20 loss: 0.0247\n",
            "Batch 40 loss: 0.0269\n",
            "Epoch 7/10, Loss: 0.0505\n",
            "Batch 20 loss: 0.0003\n",
            "Batch 40 loss: 0.0000\n",
            "Epoch 8/10, Loss: 0.0587\n",
            "Batch 20 loss: 0.0202\n",
            "Batch 40 loss: 0.0045\n",
            "Epoch 9/10, Loss: 0.0245\n",
            "Batch 20 loss: 0.0000\n",
            "Batch 40 loss: 0.0002\n",
            "Epoch 10/10, Loss: 0.0496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4307eb35",
        "outputId": "a97acefb-76a4-4b08-9a51-ab5e8209c43d"
      },
      "source": [
        "alex_net_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = alex_net_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "avg_test_loss = test_loss / total_samples\n",
        "accuracy = (correct_predictions / total_samples) * 100\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0526\n",
            "Test Accuracy: 98.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create virtual environment\n",
        "# These commands are meant for a local terminal, not directly in a Colab code cell.\n",
        "# In Colab, dependencies are typically installed directly into the environment using '!pip install'.\n",
        "# The virtual environment creation and activation are not applicable in Colab's typical workflow.\n",
        "\n",
        "# Install dependencies\n",
        "!pip install torch torchvision opencv-python\n",
        "!pip install transformers timm\n",
        "!pip install numpy pandas matplotlib\n",
        "!pip install streamlit  # Optional: for web UI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcCb5usltKDT",
        "outputId": "c5ff0001-db03-4240-d2fd-3c7732e3f951"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cpu)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.52.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.52.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.52.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Install ultralytics package if not already installed\n",
        "!pip install ultralytics\n",
        "\n",
        "# Load YOLOv5 from PyTorch Hub\n",
        "# The trust_repo=True is added to explicitly trust the repository, addressing the UserWarning.\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, trust_repo=True)\n",
        "\n",
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Test on image\n",
        "# You will need to replace 'path/to/image.jpg' with an actual image file path\n",
        "# or a numpy array/PIL Image if you want to run this example.\n",
        "# For now, let's just make sure the model loads correctly.\n",
        "# results = model('path/to/image.jpg')\n",
        "# results.show()\n",
        "print(\"YOLOv5s model loaded successfully. Ready for inference.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlt_r-kvvlY9",
        "outputId": "189fa29a-bc6b-4712-f6b4-6262134fc0af"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.240-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.240-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.240 ultralytics-thop-2.0.18\n",
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 üöÄ 2025-12-19 Python-3.12.12 torch-2.9.0+cpu CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1M/14.1M [00:00<00:00, 173MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv5s model loaded successfully. Ready for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Load model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Open webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# FPS calculation\n",
        "fps_queue = []\n",
        "\n",
        "while True:\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Read frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run inference\n",
        "    results = model(frame)\n",
        "\n",
        "    # Render results\n",
        "    rendered_frame = results.render()[0]\n",
        "\n",
        "    # Calculate FPS\n",
        "    fps = 1 / (time.time() - start_time)\n",
        "    fps_queue.append(fps)\n",
        "    if len(fps_queue) > 30:\n",
        "        fps_queue.pop(0)\n",
        "    avg_fps = sum(fps_queue) / len(fps_queue)\n",
        "\n",
        "    # Display FPS\n",
        "    cv2.putText(rendered_frame, f'FPS: {avg_fps:.1f}',\n",
        "                (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 255, 0), 2)\n",
        "\n",
        "    # Show frame\n",
        "    cv2.imshow('Object Detection', rendered_frame)\n",
        "\n",
        "    # Exit on 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KKn5IOlwcR8",
        "outputId": "4e0a15ae-9e24-41ad-cdf0-a1df8db4853e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 üöÄ 2025-12-19 Python-3.12.12 torch-2.9.0+cpu CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HTjfED2wvxc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}